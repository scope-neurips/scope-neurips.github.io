<!doctype html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <link rel="icon" href="https://pbs.twimg.com/profile_images/1242741510929612801/Mt1ozX07_400x400.jpg">
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>SCOPE Workshop @ ICLR 2025</title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
    />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

    <!-- Latest compiled and minified JavaScript -->
    <script
      src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
      integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
      crossorigin="anonymous"
    ></script>

    <link
      rel="stylesheet"
      href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css"
    />
    <!-- Custom styles for this template -->

    <link href="../css/scrolling-nav.css" rel="stylesheet" />
    <link href="../css/style.css" rel="author stylesheet" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-0M445FTS98"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-0M445FTS98");
    </script>

    <style>
      #tpc p {
        margin-bottom: 0.3em;
        font-size: 0.85em;
      }
    </style>
  </head>

  <body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
      <div class="container bar-container">
        <a class="title-head" href="/index.html#page-top">SCOPE Workshop @ ICLR 2025</a>
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarResponsive"
          aria-controls="navbarResponsive"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="/index.html#about">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="/index.html#cfp">CFP</a>
            </li>

            <li class="nav-item"></li>
              <a class="nav-link js-scroll-trigger" href="/index.html#schedule">Schedule</a>
            </li>

            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="/index.html#speakers"
                >Invited Speakers</a
              >
            </li>
            <li class="nav-item">
                <a class="nav-link js-scroll-trigger" href="/accepted_paper.html#papers"
                  >Accepted Papers</a
                >
              </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="/index.html#organizers"
                >Organizers</a
              >
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="/index.html#sponsors"
                >Sponsors</a
              >
            </li>
            <!-- <li class="nav-item"></li>
              <a class="nav-link js-scroll-trigger" href="/index.html#tpc"
                >TPC Members</a
              >
            </li> -->
          </ul>
        </div>
      </div>
    </nav>

    <header
      class="headercontainer bg-primary text-white"
      style="padding: 0%; max-height: none"
    >
      <div style="background-color: rgba(160, 160, 160, 0)" class="text-center">
        <div
          style="
            padding-bottom: 6%;
            padding-top: 6%;
            background-image: url(&quot;../images/Singapore.jpg&quot;);
            background-size: cover;
            background-position: center;
          "
        >
          <div
            class="container titlebox"
            ;
            style="
              display: inline-block;
              background-color: rgba(0, 0, 0, 0.7);
              width: auto;
            "
          >
            <p style="text-align: center; margin-bottom: 2" class="title">
              ICLR 2025 Workshop on
              <br />
              <u>Sc</u>alable <u>Op</u>timization for <u>E</u>fficient
              and Adaptive Foundation Models
              <br />
              <strong>(SCOPE)</strong>
            </p>
            <p
              style="text-align: center; margin-bottom: 0; font-size: 1em"
              class="subtitle"
            >
              <!-- Placeholder -- one sentence descroption. -->
            </p>
            <br />
            <p style="text-align: center; margin-bottom: 0" class="subtitle">
              Monday, April 28th, 2025
            </p>
            <p style="text-align: center; margin-bottom: 0" class="subtitle">
              collocated with
              <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>

              in Singapore
            </p>
          </div>
        </div>
      </div>
    </header>


    <section id="papers">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class="titlesec">Accepted Papers</span><br />

            <p> The full list of accepted papers can be found in <a href="https://openreview.net/group?id=ICLR.cc/2025/Workshop/SCOPE&referrer=%5BHomepage%5D(%2F)#tab-accept-oral">SCOPE - OpenReview</a>.</p>

            <div class="row">
              <h3>Oral Accepts</h3>
              <div class="col-md-12">
                <p><b>Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</b><br>
                  Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, Yu Cheng</p>

                  <p><b>STIV: Scalable Text and Image Conditioned Video Generation</b><br>
                  Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, Cha Chen, Yiran Fei, Yifan Jiang, Lezhi Li, Yizhou Sun, Kai-Wei Chang, Yinfei Yang</p>

                  <p><b>Overtrained Language Models Are Harder to Fine-Tune</b><br>
                  Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, Aditi Raghunathan</p>

                  <p><b>LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree Drafting for Visual Auto-regressive Models</b><br>
                  Sihwan Park, Doohyuk Jang, Sung-Yub Kim, Souvik Kundu, Eunho Yang</p>

                  <p><b>ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals</b><br>
                  Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang</p>

                  <p><b>SageAttention2: Efficient Attention with Smoothing Q and Per-thread Quantization</b><br>
                  Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen</p>

                  <p><b>Towards Infinite-Long Prefix in Transformers</b><br>
                  Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang</p>

                  <p><b>M2R2: Efficient Transformers with Mixture of Multi-Rate Residuals</b><br>
                  Nikhil Bhendawade, Mahyar Najibi, Devang Naik, Irina Belousova</p>

                  <p><b>Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity</b><br>
                  Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu</p>
              </div>

              <h3>Poster Accepts</h3>
              <div class="col-md-12">
                <p><b>Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</b><br>
                    Andy Zhou, Ron Arel</p>

                    <p><b>Margin-aware Preference Optimization for Aligning Diffusion Models without Reference</b><br>
                    Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, Jongheon Jeong</p>

                    <p><b>The Curse of Depth in Large Language Models</b><br>
                    Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu</p>

                    <p><b>DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs</b><br>
                    Zhen Tan, Daize Dong, Xinyu Zhao, Jianing Cai, Jie Peng, Yu Cheng, Tianlong Chen</p>

                    <p><b>QMambaExtend: Improving Long-Context Extension of Memory-Efficient Mamba Models</b><br>
                    Seyedarmin Azizi, Souvik Kundu, Mohammad Erfan Sadeghi, Massoud Pedram</p>

                    <p><b>Efficient Open-set Test Time Adaptation of Vision Language Models</b><br>
                    Manogna Sreenivas, Soma Biswas</p>

                    <p><b>Effortless Efficiency: Low-Cost Pruning of Diffusion Models</b><br>
                    Yang Zhang, Er Jin, Yanfei Dong, Ashkan Khakzar, Philip Torr, Johannes Stegmaier, Kenji Kawaguchi</p>

                    <p><b>Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam</b><br>
                    Tianjin Huang, Haotian Hu, Zhenyu Zhang, Gaojie Jin, Xiang Li, Li Shen, Tianlong Chen, Lu Liu, Qingsong Wen, Zhangyang Wang, Shiwei Liu</p>

                    <p><b>SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training</b><br>
                    Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu</p>

                    <p><b>Revisiting Associative Recall in Modern Recurrent Models</b><br>
                    Destiny Okpekpe, Antonio Orvieto</p>

                    <p><b>PENCIL: Long Thoughts with Short Memory</b><br>
                    Chenxiao Yang, Nathan Srebro, David McAllester, Zhiyuan Li</p>

                    <p><b>Universal LLM Routing with Correctness-Based Representation</b><br>
                    Wittawat Jitkrittum, Harikrishna Narasimhan, Ankit Singh Rawat, Jeevesh Juneja, Zifeng Wang, Chen-Yu Lee, Pradeep Shenoy, Rina Panigrahy, Aditya Krishna Menon, Sanjiv Kumar</p>

                    <p><b>AsymLoRA: Unlocking the Power of Multimodal LLMs via Asymmetric LoRA</b><br>
                    Xuyang Wei, Chunlin Tian, Li Li</p>

                    <p><b>Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention</b><br>
                    Zhendong Zhang</p>

                    <p><b>Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2</b><br>
                    Steven Abreu, Sumit Bam Shrestha, Rui-Jie Zhu, Jason Eshraghian</p>

                    <p><b>Graph Low-Rank Adapters of High Regularity for Graph Neural Networks and Graph Transformers</b><br>
                    Pantelis Papageorgiou, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios, Michael M. Bronstein</p>

                    <p><b>MixER: Better Mixture of Experts Routing for Hierarchical Meta-Learning</b><br>
                    Roussel Desmond Nzoyem, Grant Stevens, Amarpal Sahota, David A.W. Barton, Tom Deakin</p>

                    <p><b>Adaptive Length Image Tokenization via Recurrent Allocation</b><br>
                    Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman</p>

                    <p><b>Fixed-Point RNNs: From Diagonal to Dense in a Few Iterations</b><br>
                    Sajad Movahedi, Felix Sarnthein, Nicola Muca Cirone, Antonio Orvieto</p>

                    <p><b>Relevance Isn't All You Need: Scaling RAG Systems With Inference-Time Compute Via Multi-Criteria Reranking</b><br>
                    Will LeVine, Bijan Varjavand</p>

                    <p><b>Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters</b><br>
                    Kevin Li, Sachin Goyal, João D. Semedo, J Zico Kolter</p>

                    <p><b>Yes, Q-learning Helps Offline In-Context RL</b><br>
                    Denis Tarasov, Alexander Nikulin, Ilya Zisman, Albina Klepach, Andrei Polubarov, Lyubaykin Nikita, Alexander Derevyagin, Igor Kiselev, Vladislav Kurenkov</p>

                    <p><b>FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models</b><br>
                    Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma</p>

                    <p><b>ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters</b><br>
                    Kamer Ali Yuksel, Hassan Sawaf</p>

                    <p><b>A Unified Approach to Routing and Cascading for LLMs</b><br>
                    Jasper Dekoninck, Maximilian Baader, Martin Vechev</p>

                    <p><b>Conformal Transformations for Symmetric Power Transformers</b><br>
                        Saurabh Kumar, Jacob Buckman, Carles Gelada, Xiaowen Zhang</p>

                    <p><b>Training Domain Draft Models for Speculative Decoding: Best Practices and Insights</b><br>
                    Fenglu Hong, Ravi Shanker Raju, Jonathan Lingjie Li, Bo Li, Urmish Thakker, Avinash Ravichandran, Swayambhoo Jain, Changran Hu</p>

                    <p><b>Grams: Gradient Descent with Adaptive Momentum Scaling</b><br>
                    Yang Cao, Xiaoyu Li, Zhao Song</p>

                    <p><b>Fast Gradient Computation for RoPE Attention in Almost Linear Time</b><br>
                    Yifang Chen, Jiayan Huo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song</p>

                    <p><b>Domain-Invariant Prompt Learning for Vision-Language Models</b><br>
                    Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt</p>

                    <p><b>Low-Rank Continual Personalization of Diffusion Models</b><br>
                    Łukasz Staniszewski, Katarzyna Zaleska, Kamil Deja</p>

                    <p><b>Efficient Distributed Optimization under Heavy-Tailed Noise</b><br>
                    Su Hyeong Lee, Manzil Zaheer, Tian Li</p>

                    <p><b>Enhanced Continual Learning of Vision-Language Models with Model Fusion</b><br>
                    Haoyuan Gao, Zicong Zhang, Yuqi Wei, Linglan Zhao, Guilin Li, Yexin Li, Linghe Kong, Weiran Huang</p>

                    <p><b>RecurFormer: Not All Transformer Heads Need Self-Attention</b><br>
                    Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang</p>

                    <p><b>XAMBA: Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units</b><br>
                    Arghadip Das, Arnab Raha, Shamik Kundu, Soumendu Kumar Ghosh, Deepak Mathaikutty, Vijay Raghunathan</p>

                    <p><b>DARS: Robust Sparse Fine-Tuning with Regularized Subspace Disalignment</b><br>
                    Sumin Park, Noseong Park</p>

                    <p><b>In-batch Ensemble Drafting: Robust Speculative Decoding for LVLMs</b><br>
                    Minjae Lee, Wonjun Kang, Byeongkeun Ahn, Christian Classen, Minghao Yan, Hyung Il Koo, Kangwook Lee</p>

                    <p><b>OPPA: Optimizing Parallelism for Language Model Training</b><br>
                    Apivich Hemachandra, Yizhan Han, See-Kiong Ng, Bryan Kian Hsiang Low</p>

                    <p><b>N-Gram Induction Heads for In-Context RL: Improving Stability and Reducing Data Needs</b><br>
                    Ilya Zisman, Alexander Nikulin, Viacheslav Sinii, Denis Tarasov, Lyubaykin Nikita, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov</p>

                    <p><b>Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning</b><br>
                    Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horváth, Praneeth Vepakomma</p>

                    <p><b>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</b><br>
                    Abdelhakim Benechehab, Vasilii Feofanov, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl</p>

                    <p><b>UniForm: A Reuse Attention Mechanism for Efficient Transformers on Resource-Constrained Edge Devices</b><br>
                    Seul-Ki Yeom, Tae-Ho Kim</p>

                    <p><b>KV Prediction for Improved Time to First Token</b><br>
                    Maxwell Horton, Qingqing Cao, Chenfan Sun, Yanzi Jin, Sachin Mehta, Mohammad Rastegari, Moin Nabi</p>

                    <p><b>Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing</b><br>
                    Aviv Bick, Tobias Katsch, Nimit Sharad Sohoni, Arjun D Desai, Albert Gu</p>

                    <p><b>On Vanishing Variance in Transformer Length Generalization</b><br>
                    Ruining Li, Gabrijel Boduljak, Jensen Zhou</p>

                    <p><b>Attention Is All You Need For Mixture-of-Depths Routing</b><br>
                    Advait Gadhikar, Souptik Kumar Majumdar, Niclas Popp, Piyapat Saranrittichai, Martin Rapp, Lukas Schott</p>

                    <p><b>Context Is All You Need: Efficient Retrieval Augmented Generation for Domain Specific AI</b><br>
                    Peixi Xiong, Chaunte W. Lacewell, Sameh Gobriel, Nilesh Jain</p>

              </div>

            </div>
      </div>
    </section>

    <!-- Footer -->

    <!-- Bootstrap core JavaScript -->
    <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	<!-- Plugin JavaScript -->
    <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
  </body>
</html>
